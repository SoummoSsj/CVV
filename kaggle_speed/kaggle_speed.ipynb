# Predict on one sequence and export JSON
# Set SEQ_MP4 and SEQ_PKL in the first cell before running this
assert SEQ_MP4 is not None and SEQ_PKL is not None, "Set SEQ_MP4 and SEQ_PKL paths"

ckpt = torch.load('/kaggle/working/speed_reg.pt', map_location='cpu')
reg = SpeedRegressor(in_dim=3, hidden=64)
reg.load_state_dict(ckpt['state_dict'])
reg.eval()
x_mean, x_std = ckpt['x_mean'], ckpt['x_std']

model_det = YOLO('yolov8n.pt')

gt = load_bcs_gt(SEQ_PKL)
fps = float(gt.get('fps', 25.0))
lines = [np.array(l, dtype=np.float64) for l in gt.get('measurementLines', [])]
spacing_m = find_spacing_meters(gt, LINE_LO, LINE_HI)

cap = cv2.VideoCapture(SEQ_MP4)
assert cap.isOpened()

tracks = {}
next_id = 1

cars = []
frame_idx = 0

while True:
    ok, frame = cap.read()
    if not ok:
        break
    yres = model_det.predict(frame, conf=0.3, verbose=False)[0]
    boxes = yres.boxes.xyxy.cpu().numpy() if yres.boxes is not None else []
    clses = yres.boxes.cls.cpu().numpy().astype(int) if yres.boxes is not None else []
    dets = [b for b,c in zip(boxes, clses) if c in [2,3,5,7]]

    # greedy match
    used = set()
    new_tracks = {}
    for tid, t in tracks.items():
        best = (-1, 0.0)
        for j, d in enumerate(dets):
            if j in used:
                continue
            # IOU
            x1 = max(t['bbox'][0], d[0]); y1 = max(t['bbox'][1], d[1])
            x2 = min(t['bbox'][2], d[2]); y2 = min(t['bbox'][3], d[3])
            inter = max(0,x2-x1)*max(0,y2-y1)
            ua = (t['bbox'][2]-t['bbox'][0])*(t['bbox'][3]-t['bbox'][1]) + (d[2]-d[0])*(d[3]-d[1]) - inter + 1e-6
            v = inter/ua
            if v > best[1]:
                best = (j, v)
        if best[0] >= 0 and best[1] >= 0.3:
            j = best[0]
            d = dets[j]
            bc = bottom_center(d)
            new_tracks[tid] = {'bbox': d, 'frames': t['frames']+[frame_idx],
                                'pts': t['pts']+[bc.tolist()], 'h': t['h']+[float(d[3]-d[1])]}
            used.add(j)
    for j, d in enumerate(dets):
        if j in used:
            continue
        bc = bottom_center(d)
        new_tracks[next_id] = {'bbox': d, 'frames':[frame_idx], 'pts':[bc.tolist()], 'h':[float(d[3]-d[1])]}
        next_id += 1
    tracks = new_tracks
    frame_idx += 1

cap.release()

# finalize per-track speeds
for tid, t in tracks.items():
    if len(t['frames']) < 6:
        continue
    pts = np.array(t['pts'], dtype=np.float64)
    crosses = line_cross_times(t['frames'], pts, lines, fps)
    if LINE_LO not in crosses or LINE_HI not in crosses:
        continue
    dt = abs(crosses[LINE_HI] - crosses[LINE_LO])
    if dt <= 0:
        continue
    v_geom = spacing_m / dt * 3.6
    h_med = float(np.median(t['h']))
    track_len = len(t['frames'])
    x = np.array([[v_geom, h_med, track_len]], dtype=np.float32)
    xn = (x - x_mean) / (x_std + 1e-6)
    with torch.no_grad():
        dv = float(reg(torch.from_numpy(xn)).cpu().numpy().reshape(-1)[0])
    v_hat = max(0.0, v_geom + dv)

    cars.append({
        'id': int(tid),
        'frames': t['frames'],
        'posX': [float(p[0]) for p in t['pts']],
        'posY': [float(p[1]) for p in t['pts']],
        'speed_kmh': float(v_hat),
        'intersections': [{'measurementLineId': int(k), 'videoTime': float(v)} for k, v in sorted(crosses.items())]
    })

with open('/kaggle/working/SEQ.json', 'w') as f:
    json.dump({'camera_calibration': {}, 'cars': cars}, f)

'/kaggle/working/SEQ.json'# Train the regressor
X_np = np.array(X, dtype=np.float32)
Y_np = np.array(Y, dtype=np.float32).reshape(-1,1)

x_mean = X_np.mean(axis=0)
x_std = X_np.std(axis=0) + 1e-6
Xn = (X_np - x_mean) / x_std

model = SpeedRegressor(in_dim=Xn.shape[1], hidden=64)
opt = torch.optim.AdamW(model.parameters(), lr=1e-3)
loss_fn = torch.nn.L1Loss()

bs = 64
dataset = torch.utils.data.TensorDataset(torch.from_numpy(Xn), torch.from_numpy(Y_np))
loader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)

for epoch in range(10):
    total = 0.0
    for xb, yb in loader:
        opt.zero_grad()
        pred = model(xb)
        loss = loss_fn(pred, yb)
        loss.backward()
        opt.step()
        total += float(loss.item()) * len(xb)
    print(f"Epoch {epoch+1}/10 MAE(residual)={total/len(dataset):.3f}")

ckpt = {'state_dict': model.state_dict(), 'x_mean': x_mean, 'x_std': x_std}
torch.save(ckpt, '/kaggle/working/speed_reg.pt')
print('Saved /kaggle/working/speed_reg.pt')# Define the residual speed model (MLP)
class SpeedRegressor(torch.nn.Module):
    def __init__(self, in_dim=3, hidden=64):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(in_dim, hidden), torch.nn.ReLU(),
            torch.nn.Linear(hidden, hidden), torch.nn.ReLU(),
            torch.nn.Linear(hidden, 1)
        )
    def forward(self, x):
        return self.net(x)
import torch
from ultralytics import YOLO

# Line crossing helpers (mirror train script)
def line_cross_times(times, pts_xy, lines, fps):
    times_s = {}
    P = np.c_[pts_xy, np.ones((len(pts_xy), 1))]
    for li, l in enumerate(lines):
        s = P @ l
        for k in range(len(s) - 1):
            if s[k] == 0:
                times_s[li] = times[k] / fps
                break
            if s[k] * s[k + 1] < 0:
                alpha = -s[k] / (s[k + 1] - s[k] + 1e-9)
                t_cross = (times[k] + alpha) / fps
                times_s[li] = float(t_cross)
                break
    return times_s

def load_bcs_gt(pkl_path):
    with open(pkl_path, 'rb') as f:
        try:
            data = pickle.load(f, encoding='latin1')
        except TypeError:
            data = pickle.load(f)
    return data

def find_spacing_meters(gt, line_lo, line_hi):
    spacings = []
    for car in gt.get('cars', []):
        inter = {x['measurementLineId']: x['videoTime'] for x in car.get('intersections', [])}
        if line_lo in inter and line_hi in inter:
            dt = abs(inter[line_hi] - inter[line_lo])
            if dt <= 0:
                continue
            v_mps = float(car.get('speed', 0.0)) / 3.6
            spacings.append(v_mps * dt)
    if not spacings:
        raise RuntimeError('Cannot derive spacing from GT')
    return float(np.median(spacings))

def bottom_center(box):
    x1, y1, x2, y2 = box
    return np.array([(x1 + x2) / 2.0, y2], dtype=np.float32)